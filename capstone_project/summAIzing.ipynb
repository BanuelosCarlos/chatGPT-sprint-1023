{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import os\n",
    "import pydub\n",
    "from pydub.utils import which\n",
    "from pydub import AudioSegment\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSegment.converter = which(\"ffmpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio():\n",
    "    url = input(\"Introduce the YouTube URL:\")\n",
    "    try:\n",
    "        if \"https://www.youtube.com/\" in url:\n",
    "            try:\n",
    "                yt = YouTube( \n",
    "                str(url)) \n",
    "            except Exception as ee:\n",
    "                print(ee)\n",
    "\n",
    "            # extract only audio \n",
    "            audio = yt.streams.filter(only_audio=True).first() \n",
    "        \n",
    "            destination = '.'\n",
    "        \n",
    "            # download the file \n",
    "            out_file = audio.download(output_path=destination) \n",
    "        \n",
    "            # save the file \n",
    "            base, ext = os.path.splitext(out_file) \n",
    "            new_file = (base + '.mp3').replace(\" \", \"_\")\n",
    "            os.rename(out_file, new_file) \n",
    "        \n",
    "            # result of success \n",
    "            print(yt.title + \" has been successfully downloaded.\")\n",
    "            return new_file\n",
    "        else:\n",
    "            print(\"This is not a valid URL!\")\n",
    "    except Exception as ee:\n",
    "        print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build a Chatbot in Python| ChatGPT API Complete Tutorial for Beginners has been successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "file_name = download_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_size(file_location):\n",
    "    file_stats = os.stat(file_location)\n",
    "    file_size = file_stats.st_size / (1024*1024) # In MB\n",
    "    print(f\"Audio file: {file_size:.2f} MB\")\n",
    "    return file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(file_name):\n",
    "    try:\n",
    "        mp3_audio = pydub.AudioSegment.from_file(file_name, \"mp3\")\n",
    "    except Exception as ee:\n",
    "        mp3_audio = pydub.AudioSegment.from_file(file_name, \"mp4\")\n",
    "\n",
    "    chunk_size = 10 * 1024 * 1024  # 10 MB in bytes\n",
    "    num_chunks = len(mp3_audio) // chunk_size + 1\n",
    "    \n",
    "    chunk_names = []\n",
    "    for counter in range(num_chunks):\n",
    "        start = counter * chunk_size\n",
    "        end = min(start + chunk_size, len(mp3_audio))\n",
    "        chunk = mp3_audio[start:end]\n",
    "        chunk_name = \"output_\" + str(counter + 1) + \".mp3\"\n",
    "        chunk.export(chunk_name, format=\"mp3\")\n",
    "        chunk_names.append(chunk_name)\n",
    "    return chunk_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file: 2.85 MB\n",
      "Correct size!\n"
     ]
    }
   ],
   "source": [
    "file_size = check_audio_size(file_name)\n",
    "if file_size > 20:\n",
    "    chunk_names = split_audio(file_name)\n",
    "else:\n",
    "    chunk_names = [file_name]\n",
    "    print('Correct size!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(num_chunks):\n",
    "    start = counter * chunk_size\n",
    "    end = min(start + chunk_size, len(mp3_audio))\n",
    "    chunk = mp3_audio[start:end]\n",
    "    chunk.export(\"output_\" + str(counter + 1) + \".mp3\", format=\"mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone. My name is Nachiketa and welcome back to another video. In this video, I'm going to show you how you can use ChatGPT to create your own personal chatbot for any of your application. You might have already heard a lot about ChatGPT. Just to summarize, ChatGPT is basically an AI-powered chatbot which uses something called as GPT or basically a generative pre-trained transformer, which is basically a language model which allows you to talk to it like a human does. But if you're building an application, let's say you're building a website, you're building a mobile phone app and you want to build a chatbot into it, you don't want the end-user to directly go into the ChatGPT website and enter, let's say, a prompt and get the answer himself. You want them to be able to do it from your application itself. Now, since you don't want the end-user to go to ChatGPT himself, what ChatGPT has done is they have created a public API using which, from your application, you can take inputs from a user and you can send it to the API and get the ChatGPT's response in return. Now, the power of that is when you do it from your application, you will have access to the application data and user data using which you can create more powerful responses which ChatGPT couldn't do. So in this video, I'm going to show you how you can use the API to build your own chatbot very easily in Python. So let's straightaway get started. The first thing you'll have to do is you have to go to the OpenAI website. I'll give a link for this in the description and you have to create your own API key by clicking on the Create New Secret Key option. I've already done that. Once you create a key, copy that and save that somewhere, right, because you only get to see it once. Moreover, I'll leave a link for the OpenAI's official guide to use the chat completion API. So you can go through it. I'll be summarizing this in a very quick manner in this particular video. The API is also paid and you have to pay upon the usage. So I'm going to also show you ways how you can minimize the cost as well. So the first thing we do is we install the OpenAI library using the pip install OpenAI command and then over here we import it. The next thing you have to do is you have to provide your OpenAI API key. Right now, I've put in a placeholder over here, but please make sure that the API key you get from OpenAI, that is what you have to paste over here. So we first create a list called as messages. Inside this, you'll find a dictionary which has two particular key values. One is the role, one is the content and this is how you communicate with the chat GPT API and over here when I say the role is system, I'm basically acting as a system to instruct chat GPT how to behave and you can see this in the official documentation as well, right, that the system helps set the behavior of the assistant. So over here we're writing you are an intelligent assistant. However, currently the documentation also states that it does not pay a strong attention to the system messages. Maybe in the future we'll start doing that. Now what we're going to do is we're running a while loop because what we want to do is we want a conversation in which the user will ask, where the user will ask questions, chat GPT will reply and the conversation can continue. We just don't want a single reply from chat GPT. We want a conversation. That's why we create a while loop. You can put any exit condition. If you want, let's say a number of responses after which you want to exit that's dependent upon you. Next we take an input from the user and if a user has given a particular input, we append a new dictionary in the messages list that we had created over here. This time the role will be user which basically means that the user is asking something and the content will be whatever message or input was given by the user. Now we call the open AI dot chat completion dot create function which basically takes mainly two things. One is the messages array that we had created and which will contain the system instruction as well as what the user is asking and you have to specify a model. So but the model we're using is GPT 3.5 in the official documentation. You can find a list of different models which are specified for different tasks. It's best to use GPT 3.5 right now, which will help you do most tasks of natural language and it's pretty cheap as well. There are different models specified for different tasks like particular models are better for images particular models are better for code completion, but the pricing is something you have to check. They have also given GPT 4, but this GPT 4 does not accept images right now because it's in a particular beta stage in the future. You will also be able to use GPT 4 and give it images from the user right now. You can't do that. So once I specify a model you give it the user messages. There's also a particular parameter called as temperature which basically determines how much creative you want the responses to be. So if you increase the value can be between 0 to 1 0 means low risk low creativity and 1 means high risk high creativity. Once you call this API the charge API will return the charge UD's response and it will return the response in a format like this. So inside a particular key called choices, you will find the message embedded from charge you will like this to extract that here is what we have to do we in chat. We extract the first element from choices and in message dot content is the actual response of chatbot. We print that over here and there's also a very important line. Whatever reply charge UD give us we're appending that back again into this particular list, right and we are providing the role as assistant. So what is happening is as the while loop is running it is keeping a track of all the user inputs as well as charge UD's responses because you can see the reply is stored also in this particular dictionary and we're appending that. So as the conversation continues this will help the chatbot keep track of the contest context of the communication will remember the history. So I just run that and once I run this you will find text input box to be given to you. I'm going to say I need to summarize. Computer vision in 20 words. And so to give computer vision as a field of AI that enables computers to interpret and understand visual data. How is it different from traditional image processing? So I just ask this question so I can show you can keep track of the conversation. So I can see the response over here that it's different from traditional image processing and that uses machine learning algorithms, right? Whereas traditional image processing does not. So you can see that it keeps a track of the conversation and it remembers history as and context as well. So you're charged on the number of tokens. So for each sentence, for example, if I ask how are you question mark that will be four tokens. If I just ask how are you that will be three tokens. So I'll be charged less on my second question. Similarly, you are also charged on the number of tokens returned by chat GPT. So I can in my first instruction. I can just say summarize all responses in less than 50 words or keep the number of tokens to a minimum. Like I said in my first line, right? That is something you can do to keep the cost lower and you can use when you are using a particular model over here. Just see what pricing it has and prefer a cheaper model if you just experimenting with things. So that's all for this video. If you did like this do like share and subscribe to this channel and let me know what you want to see in my future videos. I'll be talking more about this API and Lanchain frameworks, which allows you to build really powerful customized applications for any text completion tasks, right? There are lots of cool websites and apps you can build and I'll be making a lot more videos on that till then. Thank you for watching and see you again in the next video.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(new_file, \"rb\"),\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Video topic\": \"Using ChatGPT for building personal chatbots\",\n",
      "  \"Summary\": \"The video provides a tutorial on how to use the ChatGPT public API to build a personalized chatbot for websites or mobile apps. The speaker explains the process of creating personalized responses based on user data and API usage to control costs. The tutorial includes steps such as creating an OpenAI API key, using the library, setting up messages for communication, and controlling costs by managing the number of tokens used.\",\n",
      "  \"Keywords\": \"ChatGPT, chatbot, public API, OpenAI, AI applications, user data, library, API usage control, tokens\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "doc_content = transcript.text\n",
    "prompt = f\"\"\"   \n",
    "    It is necessary to generate a detailed summary about the following text \n",
    "    that is extracted from a youtube video:\n",
    "    '''\n",
    "        {doc_content}\n",
    "    '''\n",
    "    Please return the summary in a json format containing the following fields:\n",
    "    '''\n",
    "        \"Video topic\": ...\n",
    "        \"Summary\": ...\n",
    "        \"Keywords\": ...\n",
    "    '''\n",
    "\"\"\"\n",
    "\n",
    "#response = openai.ChatCompletion.create(\n",
    "#    model = 'gpt-3.5-turbo',\n",
    "#    messages = [\n",
    "#        {\n",
    "#            \"role\": \"user\",\n",
    "#            \"content\": prompt\n",
    "#        }\n",
    "#    ]\n",
    "#)\n",
    "#print(response['choices'][0]['messages']['content'])\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt \n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=1.9,\n",
    ")\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AudioSegment' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     mp3_audio \u001b[39m=\u001b[39m pydub\u001b[39m.\u001b[39;49mAudioSegment\u001b[39m.\u001b[39;49mfrom_file(input_file, \u001b[39m\"\u001b[39;49m\u001b[39mmp3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ee:\n",
      "File \u001b[0;32m~/Documents/chatGPT-project/chatGPT-sprint-1023/openai-venv/lib/python3.10/site-packages/pydub/audio_segment.py:723\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m     stdin_parameter \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE\n\u001b[0;32m--> 723\u001b[0m     stdin_data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m codec:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AudioSegment' object has no attribute 'read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         chunk_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_folder, chunk_filename)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         chunk\u001b[39m.\u001b[39mexport(chunk_path, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmp3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m split_mp3_file(mp3_audio)\n",
      "\u001b[1;32m/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     mp3_audio \u001b[39m=\u001b[39m pydub\u001b[39m.\u001b[39mAudioSegment\u001b[39m.\u001b[39mfrom_file(input_file, \u001b[39m\"\u001b[39m\u001b[39mmp3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ee:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     mp3_audio \u001b[39m=\u001b[39m pydub\u001b[39m.\u001b[39;49mAudioSegment\u001b[39m.\u001b[39;49mfrom_file(input_file, \u001b[39m\"\u001b[39;49m\u001b[39mmp4\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_chunks \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(audio) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m chunk_size \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/Documents/chatGPT-project/chatGPT-sprint-1023/capstone_project/summAIzing.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m output_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mchunks\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/chatGPT-project/chatGPT-sprint-1023/openai-venv/lib/python3.10/site-packages/pydub/audio_segment.py:723\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         conversion_command \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m-i\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    722\u001b[0m     stdin_parameter \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE\n\u001b[0;32m--> 723\u001b[0m     stdin_data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m codec:\n\u001b[1;32m    726\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AudioSegment' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_mp3_file(input_file):\n",
    "    chunk_size = 10 * 1024 * 1024 \n",
    "    try:\n",
    "        mp3_audio = pydub.AudioSegment.from_file(input_file, \"mp3\")\n",
    "    except Exception as ee:\n",
    "        mp3_audio = pydub.AudioSegment.from_file(input_file, \"mp4\")\n",
    "    num_chunks = len(input_file) // chunk_size + 1\n",
    "\n",
    "    output_folder = \"chunks\"\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min(start + chunk_size, len(input_file))\n",
    "        chunk = input_file[start:end]\n",
    "\n",
    "        chunk_filename = f\"output_{i + 1}.mp3\"\n",
    "        chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "        chunk.export(chunk_path, format=\"mp3\")\n",
    "\n",
    "\n",
    "split_mp3_file(mp3_audio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
